{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Tuple, Union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative Techniques in Matrix Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 7.3 The Jacobi and Gauss-Siedel Iterative Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jacobi Method\n",
    "$$x^{(k)}_i=\\sum^n_{\\substack{j\\neq i \\\\ j=1}}\\biggl(-\\frac{a_{ij}x^{(k)}_{j}}{a_{ii}}\\biggr)+\\frac{b_i}{a_{ii}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Jacobi(A:np.ndarray, x:np.ndarray, b:np.ndarray, iter:int, tol:float=1e-5) -> np.ndarray:\n",
    "    x_ = np.copy(x)\n",
    "    for iter in range(iter):\n",
    "        prev_x = np.copy(x_)\n",
    "        for idx in range(A.shape[0]):\n",
    "            mask = np.arange(len(x_)) != idx\n",
    "            x_[idx] = 1 / (A[idx][idx]) * (b[idx] - np.sum(A[idx][mask] @ prev_x[mask]))\n",
    "        # if np.linalg.norm(x_ - prev_x) < tol:\n",
    "        #     print(\"Iteration stopped, norm difference < tolerance.\")\n",
    "        #     break\n",
    "    return x_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gauss-Siedel Method\n",
    "$$x^{(k)}_i=\\frac{1}{a_{ii}}\\biggl[\n",
    "    -\\sum^{i-1}_{j=1}(a_{ij}x^{(k)}_j)-\\sum^{n}_{j=i+1}(a_{ij}x^{(k-1)}_j)+b_i\n",
    "\\biggr]$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GaussSeidel(A:np.ndarray, x:np.ndarray, b:np.ndarray, iter:int, tol:float=1e-5) -> np.ndarray:\n",
    "    x_ = np.copy(x)\n",
    "    for iter in range(iter):\n",
    "        prev_x = np.copy(x_)\n",
    "        for idx in range(A.shape[0]):\n",
    "            mask = np.arange(len(x_)) != idx\n",
    "            x_[idx] = 1 / (A[idx][idx]) * (b[idx] - np.sum(A[idx][mask] @ x_[mask]))\n",
    "        # if np.linalg.norm(x_ - prev_x) < tol:\n",
    "        #     print(\"Iteration stopped, norm difference < tolerance.\")\n",
    "        #     break\n",
    "    return x_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 7.4 Relaxation Techniques for Solving Linear Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sucessive Over-Relaxation Method\n",
    "$$x^{(k)}_i=(1-\\omega)x^{(k-1)}_i+\\frac{\\omega}{a_{ii}}\\biggl[\n",
    "    -\\sum^{i-1}_{j=1}(a_{ij}x^{(k)}_j)-\\sum^{n}_{j=i+1}(a_{ij}x^{(k-1)}_j)+b_i\n",
    "\\biggr]$$\n",
    "where $\\omega > 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SOR(A:np.ndarray, x:np.ndarray, b:np.ndarray, iter:int, omega=1.5, tol:float=1e-5) -> np.ndarray:\n",
    "    x_ = np.copy(x)\n",
    "    for iter in range(iter):\n",
    "        prev_x = np.copy(x_)\n",
    "        for idx in range(A.shape[0]):\n",
    "            mask = np.arange(len(x_)) != idx\n",
    "            x_[idx] = (1 - omega) * prev_x[idx] + (omega / (A[idx][idx]) * (b[idx] - np.sum(A[idx][mask] @ x_[mask])))\n",
    "        # if np.linalg.norm(x_ - prev_x) < tol:\n",
    "        #     print(\"Iteration stopped, norm difference < tolerance.\")\n",
    "        #     break\n",
    "    return x_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 7.6 The Conjugate Gradient Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot(alpha, beta):\n",
    "    return np.dot(alpha.flatten(), beta.flatten())\n",
    "\n",
    "# def ConjugateGradient(A:np.ndarray, x:np.ndarray, b:np.ndarray, v:np.ndarray, tol=1e-6) -> np.ndarray:\n",
    "#     x_ = x.copy().astype(float)\n",
    "#     for k in range(v.shape[0]):\n",
    "#         t = np.dot(v[:, k], b - A @ x_) / (np.dot(v[:, k], A @ v[:, k]))\n",
    "#         x_ += (t * v[:, k]).reshape(x_.shape)\n",
    "#         if np.linalg.norm(b - A @ x_) < tol:\n",
    "#             return x_\n",
    "#     return x_\n",
    "\n",
    "def ConjugateGradient(A:np.ndarray, x:np.ndarray, b:np.ndarray, n:int=None, tol=1e-6) -> np.ndarray:\n",
    "    x_ = x.copy().astype(float)\n",
    "    r = b - A @ x_\n",
    "    v = r.copy()\n",
    "\n",
    "    if n is None:\n",
    "        n = int(len(b) * 10)\n",
    "\n",
    "    for _ in range(n):\n",
    "        if np.linalg.norm(r) < tol:\n",
    "            return x_\n",
    "        s = 1 / dot(r, r)\n",
    "        t = dot(r, r) / dot(v, A @ v)\n",
    "        x_ += (t * v).reshape(x_.shape)\n",
    "        r -= t * A @ v\n",
    "        s *= dot(r, r)\n",
    "        v = r + s * v\n",
    "\n",
    "    return x_\n",
    "\n",
    "def PreconditionConjugateGradient(A:np.ndarray, C:np.ndarray, x:np.ndarray, b:np.ndarray, n:int=None, tol=1e-6) -> np.ndarray:\n",
    "    x_ = x.copy().astype(float)\n",
    "    r = b - A @ x_\n",
    "    w = C @ r\n",
    "    v = C.T @ w\n",
    "    if n is None:\n",
    "        n = int(len(b) * 10)\n",
    "\n",
    "    for _ in range(n):\n",
    "        if np.linalg.norm(v) < tol:\n",
    "            return x_\n",
    "        s = 1 / dot(w, w)\n",
    "        t = dot(w, w) / dot(v, A @ v)\n",
    "        x_ += (t * v).reshape(x_.shape)\n",
    "        r -= t * A @ v\n",
    "        if np.linalg.norm(r) < tol:\n",
    "            return x_\n",
    "        w = C @ r\n",
    "        s *= dot(w, w)\n",
    "        v = C.T @ w + s * v\n",
    "    return x_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BiconjugateGradient(A:np.ndarray, x0:np.ndarray, b:np.ndarray, n:int=None, tol:float=1e-6) -> np.ndarray:\n",
    "    x = x0.copy()\n",
    "    r = b - A @ x\n",
    "    r_ = r.copy().flatten()\n",
    "    v, v_ = r.copy(), r_.copy()\n",
    "    if n is None:\n",
    "        n = int(len(b) * 10)\n",
    "    for _ in range(n):\n",
    "        if np.linalg.norm(r) < tol:\n",
    "            return x\n",
    "        s = 1 / dot(r_, r)\n",
    "        t = dot(r_, r) / dot(v_, A @ v)\n",
    "        x += (t * v).reshape(x.shape)\n",
    "        r -= t * A @ v\n",
    "        r_ -= t * (A.T @ v_)\n",
    "        s *= dot(r_, r)\n",
    "        v = r + s * v\n",
    "        v_ = r_ + s.conj() * v_\n",
    "    return x\n",
    "\n",
    "def PreconditionBiconjugateGradient(A:np.ndarray, C:np.ndarray, x0:np.ndarray, b:np.ndarray, n:int=None, tol=1e-6) -> np.ndarray:\n",
    "    x = x0.copy()\n",
    "    r = b - A @ x\n",
    "    r_ = r.copy().flatten()\n",
    "    v, v_ = C @ r.copy(), C.T @ r_.copy()\n",
    "    if n is None:\n",
    "        n = int(len(b) * 10)\n",
    "    for _ in range(n):\n",
    "        if np.linalg.norm(r) < tol:\n",
    "            return x\n",
    "        s = 1 / dot(r_, C @ r)\n",
    "        t = dot(r_, C @ r) / dot(v_, A @ v)\n",
    "        x += (t * v).reshape(x.shape)\n",
    "        r -= t * A @ v\n",
    "        r_ -= t * (A.T @ v_)\n",
    "        s *= dot(r_, C @ r)\n",
    "        v = C @ r + s * v\n",
    "        v_ = (C.T @ r_).flatten() + s.conj() * v_\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BiconjugateGradientSTAB(A:np.ndarray, x0:np.ndarray, b:np.ndarray, n:int=None, tol:float=1e-6) -> np.ndarray:\n",
    "    x = x0.copy()\n",
    "    r = b - A @ x\n",
    "    r_ = r.copy().flatten()\n",
    "    rho = dot(r_, r)\n",
    "    p = r.copy()\n",
    "    if n is None:\n",
    "        n = int(len(b) * 10)\n",
    "    for _ in range(n):\n",
    "        if np.linalg.norm(r) < tol:\n",
    "            return x\n",
    "        v = A @ p\n",
    "        alpha = rho / dot(r_, v)\n",
    "        r -= alpha * v\n",
    "        s = r.copy()\n",
    "        t = A @ s\n",
    "        omega = dot(t, s) / dot(t, t)\n",
    "        r -= omega * t\n",
    "        x += alpha * p + omega * s\n",
    "        beta = alpha / (omega * rho)\n",
    "        rho = dot(r_, r)\n",
    "        beta *= rho\n",
    "        p = r + beta * (p - omega * v)\n",
    "    return x\n",
    "\n",
    "def PreconditionBiconjugateGradientSTAB(A:np.ndarray, C:np.ndarray, x0:np.ndarray, b:np.ndarray, n:int=None, tol:float=1e-6) -> np.ndarray:\n",
    "    x = x0.copy()\n",
    "    r = b - A @ x\n",
    "    r_ = r.copy().flatten()\n",
    "    rho = dot(r_, r)\n",
    "    p = r.copy()\n",
    "    if n is None:\n",
    "        n = int(len(b) * 10)\n",
    "    for _ in range(n):\n",
    "        if np.linalg.norm(r) < tol:\n",
    "            return x\n",
    "        v = A @ C @ p\n",
    "        alpha = rho / dot(r_, v)\n",
    "        r -= alpha * v\n",
    "        s = r.copy()\n",
    "        t = A @ C @ s\n",
    "        omega = dot(t, s) / dot(t, t)\n",
    "        r -= omega * t\n",
    "        x += alpha * C @ p + omega * C @ s\n",
    "        beta = alpha / (omega * rho)\n",
    "        rho = dot(r_, r)\n",
    "        beta *= rho\n",
    "        p = r + beta * (p - omega * v)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MINRES(A:np.ndarray, x:np.ndarray, b:np.ndarray, tol=1e-6) -> np.ndarray:\n",
    "    x_ = x.copy().astype(float)\n",
    "    r = b - A @ x_\n",
    "    v0 = r.copy()\n",
    "    s0 = A @ v0\n",
    "    v1 = v0.copy()\n",
    "    s1 = s0.copy()\n",
    "    for k in range(x.shape[0]):\n",
    "        v2 = v1.copy()\n",
    "        v1 = v0.copy()\n",
    "        s2 = s1.copy()\n",
    "        s1 = s0.copy()\n",
    "        t = dot(r, s1) / dot(s1, s1)\n",
    "        x_ += (t * v1).reshape(x_.shape)\n",
    "        r -= t * s1\n",
    "        if np.linalg.norm(r) < tol:\n",
    "            return x_\n",
    "        s0 = A @ s1\n",
    "        beta = dot(s0, s1) / dot(s1, s1)\n",
    "        v0 = s1 - beta * v1\n",
    "        s0 -= beta * s1\n",
    "        if k:\n",
    "            beta = dot(s0, s2) / dot(s2, s2)\n",
    "            v0 -= beta * v2\n",
    "            s0 -= beta * s2\n",
    "    return x_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biconjugate Gradient\n",
      "[[ 9.084332 ]\n",
      " [-0.9101125]\n",
      " [-8.020199 ]]\n",
      "Preconditioned Biconjugate Gradient\n",
      "[[ 9.084332 ]\n",
      " [-0.9101125]\n",
      " [-8.020199 ]]\n",
      "Biconjugate Gradient Stabilized\n",
      "[[ 9.084332 ]\n",
      " [-0.9101124]\n",
      " [-8.020199 ]]\n",
      "Preconditioned Biconjugate Gradient\n",
      "[[ 9.084332 ]\n",
      " [-0.9101124]\n",
      " [-8.020199 ]]\n",
      "MINRES\n",
      "[[ 9.0843320244]\n",
      " [-0.9101124179]\n",
      " [-8.0201983357]]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(10)\n",
    "\n",
    "A = np.random.rand(3, 3)\n",
    "\n",
    "b = np.array([\n",
    "    [2],\n",
    "    [4],\n",
    "    [-1],\n",
    "])\n",
    "\n",
    "x = np.array([\n",
    "    [0.],\n",
    "    [0.],\n",
    "    [0.],\n",
    "], dtype=np.float32)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Biconjugate Gradient\")\n",
    "print(BiconjugateGradient(A, x, b))\n",
    "\n",
    "print(\"Preconditioned Biconjugate Gradient\")\n",
    "print(PreconditionBiconjugateGradient(A, np.eye(A.shape[0]), x, b))\n",
    "\n",
    "print(\"Biconjugate Gradient Stabilized\")\n",
    "print(BiconjugateGradientSTAB(A, x, b))\n",
    "\n",
    "print(\"Preconditioned Biconjugate Gradient\")\n",
    "print(PreconditionBiconjugateGradientSTAB(A, np.eye(A.shape[0]), x, b))\n",
    "\n",
    "print(\"MINRES\")\n",
    "print(MINRES(A, x, b))\n",
    "\n",
    "# print(\"Preconditioned Conjugate Gradient\")\n",
    "# print(PreconditionConjugateGradient(A, 1 / np.sqrt(np.diag(A)) * np.eye(A.shape[0]), x, b, 0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9.0843320244 -0.9101124179 -8.0201983357]\n",
      "[ 9.0843320244 -0.9101124179 -8.0201983357]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csc_matrix\n",
    "from scipy.sparse.linalg import bicg, cg, minres, bicgstab\n",
    "A_ = A.copy()\n",
    "A = csc_matrix(A)\n",
    "b = np.array([2., 4., -1.])\n",
    "x, exitCode = bicgstab(A, b, atol=1e-6, rtol=1e-6)\n",
    "print(x)\n",
    "x, exitCode = bicg(A, b, atol=1e-6, rtol=1e-6)\n",
    "print(x)\n",
    "# np.allclose(A.dot(x), b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "## Theorem 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
